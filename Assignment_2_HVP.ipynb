{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "92778525",
      "metadata": {
        "id": "92778525"
      },
      "source": [
        "# Assignment 2: Linear Models and Validation Metrics (30 marks total)\n",
        "### Due: October 10 at 11:59pm\n",
        "\n",
        "### Name:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce31b39a",
      "metadata": {
        "id": "ce31b39a"
      },
      "source": [
        "### In this assignment, you will need to write code that uses linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c6de86",
      "metadata": {
        "id": "f7c6de86"
      },
      "source": [
        "## Part 1: Classification (14.5 marks total)\n",
        "\n",
        "You have been asked to develop code that can help the user determine if the email they have received is spam or not. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e3c6fc8",
      "metadata": {
        "id": "7e3c6fc8"
      },
      "source": [
        "### Step 0: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "33f86925",
      "metadata": {
        "id": "33f86925"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9d33a8",
      "metadata": {
        "id": "5f9d33a8"
      },
      "source": [
        "### Step 1: Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/spam.html\n",
        "\n",
        "Use the yellowbrick function `load_spam()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "33583c67",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33583c67",
        "outputId": "a0c4b470-68fc-4fa3-c6d0-34015d4b1afa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of X:  (4600, 57)\n",
            "Type of X:  <class 'pandas.core.frame.DataFrame'>\n",
            "Size of y:  (4600,)\n",
            "Type of y:  <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Import spam dataset from yellowbrick library\n",
        "# TO DO: Print size and type of X and y\n",
        "\n",
        "# Import the load_spam function from the yellowbrick.datasets module\n",
        "from yellowbrick.datasets import load_spam\n",
        "\n",
        "# Load the spam dataset into X and y\n",
        "X, y = load_spam()\n",
        "\n",
        "# Print the size and type of X (features)\n",
        "\n",
        "print(\"Size of X: \", X.shape)\n",
        "\n",
        "print(\"Type of X: \", type(X))\n",
        "\n",
        "# Print the size and type of y (target labels)\n",
        "print(\"Size of y: \", y.shape)\n",
        "\n",
        "print(\"Type of y: \", type(y))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "156db208",
      "metadata": {
        "id": "156db208"
      },
      "source": [
        "### Step 2: Data Processing (1.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "4e7204f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e7204f5",
        "outputId": "00eb3aec-adbd-48f6-c29a-3278b04e6a21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word_freq_make                0\n",
            "word_freq_address             0\n",
            "word_freq_all                 0\n",
            "word_freq_3d                  0\n",
            "word_freq_our                 0\n",
            "word_freq_over                0\n",
            "word_freq_remove              0\n",
            "word_freq_internet            0\n",
            "word_freq_order               0\n",
            "word_freq_mail                0\n",
            "word_freq_receive             0\n",
            "word_freq_will                0\n",
            "word_freq_people              0\n",
            "word_freq_report              0\n",
            "word_freq_addresses           0\n",
            "word_freq_free                0\n",
            "word_freq_business            0\n",
            "word_freq_email               0\n",
            "word_freq_you                 0\n",
            "word_freq_credit              0\n",
            "word_freq_your                0\n",
            "word_freq_font                0\n",
            "word_freq_000                 0\n",
            "word_freq_money               0\n",
            "word_freq_hp                  0\n",
            "word_freq_hpl                 0\n",
            "word_freq_george              0\n",
            "word_freq_650                 0\n",
            "word_freq_lab                 0\n",
            "word_freq_labs                0\n",
            "word_freq_telnet              0\n",
            "word_freq_857                 0\n",
            "word_freq_data                0\n",
            "word_freq_415                 0\n",
            "word_freq_85                  0\n",
            "word_freq_technology          0\n",
            "word_freq_1999                0\n",
            "word_freq_parts               0\n",
            "word_freq_pm                  0\n",
            "word_freq_direct              0\n",
            "word_freq_cs                  0\n",
            "word_freq_meeting             0\n",
            "word_freq_original            0\n",
            "word_freq_project             0\n",
            "word_freq_re                  0\n",
            "word_freq_edu                 0\n",
            "word_freq_table               0\n",
            "word_freq_conference          0\n",
            "char_freq_;                   0\n",
            "char_freq_(                   0\n",
            "char_freq_[                   0\n",
            "char_freq_!                   0\n",
            "char_freq_$                   0\n",
            "char_freq_#                   0\n",
            "capital_run_length_average    0\n",
            "capital_run_length_longest    0\n",
            "capital_run_length_total      0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "\n",
        "# Calculate the number of missing values in the DataFrame 'X' and store it in 'anyMissingValue'\n",
        "\n",
        "anyMissingValue = X.isnull().sum()\n",
        "\n",
        "# Print the count of missing values\n",
        "\n",
        "print(anyMissingValue)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a489285a",
      "metadata": {
        "id": "a489285a"
      },
      "source": [
        "For this task, we want to test if the linear model would still work if we used less data. Use the `train_test_split` function from sklearn to create a new feature matrix named `X_small` and a new target vector named `y_small` that contain **5%** of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "f9bc4a23",
      "metadata": {
        "id": "f9bc4a23"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Import the train_test_split function from scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset into a small training set (X_small_train, y_small_train) and a small test set (X_small_test, y_small_test)\n",
        "\n",
        "\n",
        "X_small_train, X_small_test, y_small_train, y_small_test  = train_test_split(X, y, random_state=0, train_size=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e6c46f",
      "metadata": {
        "id": "70e6c46f"
      },
      "source": [
        "### Step 3: Implement Machine Learning Model\n",
        "\n",
        "1. Import `LogisticRegression` from sklearn\n",
        "2. Instantiate model `LogisticRegression(max_iter=2000)`.\n",
        "3. Implement the machine learning model with three different datasets:\n",
        "    - `X` and `y`\n",
        "    - Only first two columns of `X` and `y`\n",
        "    - `X_small` and `y_small`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a Logistic Regression model with a maximum of 2000 iterations\n",
        "logreg = LogisticRegression(max_iter=2000)\n",
        "\n",
        "# Split the entire dataset (X, y) into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Create a subset of the features (using the first 2 columns of X)\n",
        "X_subset = X.iloc[:, :2]\n",
        "\n",
        "# Split the subset of features and the target variable into training and testing sets\n",
        "X_train_selective, X_test_selective, y_train_selective, y_test_selective = train_test_split(X_subset, y, random_state=0)\n",
        "\n",
        "# Create a smaller training dataset (5% of the entire dataset) for faster experimentation\n",
        "X_small_train, X_small_test, y_small_train, y_small_test = train_test_split(X, y, random_state=0, train_size=0.05)\n"
      ],
      "metadata": {
        "id": "sgMZUyCVVZgk"
      },
      "id": "sgMZUyCVVZgk",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b89f3d84",
      "metadata": {
        "id": "b89f3d84"
      },
      "source": [
        "### Step 4: Validate Model\n",
        "\n",
        "Calculate the training and validation accuracy for the three different tests implemented in Step 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Training and evaluating a logistic regression model on the complete dataset\n",
        "logreg1 = LogisticRegression(max_iter=2000).fit(X_train, y_train)\n",
        "print(\"Training set score for complete set: {:.3f}\".format(logreg1.score(X_train, y_train)))\n",
        "print(\"Validation set score for complete set: {:.3f}\".format(logreg1.score(X_test, y_test)))\n",
        "print('')\n",
        "\n",
        "\n",
        "# Training and evaluating a logistic regression model on a selective subset of the dataset (contains first two columns)\n",
        "logreg2 = LogisticRegression(max_iter=2000).fit(X_train_selective, y_train_selective)\n",
        "print(\"Training set score for set(contains first two columns): {:.3f}\".format(logreg2.score(X_train_selective, y_train_selective)))\n",
        "print(\"Validation set score for set(contains first two columns): {:.3f}\".format(logreg2.score(X_test_selective, y_test_selective)))\n",
        "print('')\n",
        "\n",
        "\n",
        "# Training and evaluating a logistic regression model on a smaller subset of the dataset (contains 5% of the data)\n",
        "logreg3 = LogisticRegression(max_iter=2000).fit(X_small_train, y_small_train)\n",
        "print(\"Training set score for set(that contains 5 percentage of the data): {:.3f}\".format(logreg3.score(X_small_train, y_small_train)))\n",
        "print(\"Validation set score for set(that contains 5 percentage of the data): {:.3f}\".format(logreg3.score(X_small_test, y_small_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0yheuN8VeD8",
        "outputId": "0b355ad3-2cda-4478-96a0-24cb337a0538"
      },
      "id": "k0yheuN8VeD8",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score for complete set: 0.929\n",
            "Validation set score for complete set: 0.937\n",
            "\n",
            "Training set score for set(contains first two columns): 0.608\n",
            "Validation set score for set(contains first two columns): 0.613\n",
            "\n",
            "Training set score for set(that contains 5 percentage of the data): 0.943\n",
            "Validation set score for set(that contains 5 percentage of the data): 0.906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "352106a3",
      "metadata": {
        "id": "352106a3"
      },
      "source": [
        "### Step 5: Visualize Results (4 marks)\n",
        "\n",
        "1. Create a pandas DataFrame `results` with columns: Data size, training accuracy, validation accuracy\n",
        "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "be4b5c0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be4b5c0a",
        "outputId": "023a7c27-a89a-47c0-cab7-815de600bf08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Data size  training accuracy  validation accuracy\n",
            "0       3450           0.928696             0.937391\n",
            "1       3450           0.608406             0.613043\n",
            "2        230           0.943478             0.905721\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT\n",
        "\n",
        "# Importing LogisticRegression from scikit-learn and creating a DataFrame to store and display model evaluation results\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "data = {\n",
        "\n",
        "  'Data size' : [len(X_train), len(X_train_selective), len(X_small_train)],\n",
        "\n",
        "  'training accuracy' : [logreg1.score(X_train, y_train), logreg2.score(X_train_selective, y_train_selective), logreg3.score(X_small_train, y_small_train)],\n",
        "\n",
        "  'validation accuracy' : [logreg1.score(X_test, y_test), logreg2.score(X_test_selective, y_test_selective), logreg3.score(X_small_test, y_small_test)]\n",
        "\n",
        "}\n",
        "\n",
        "results = pd.DataFrame(data)\n",
        "\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4427d4f",
      "metadata": {
        "id": "d4427d4f"
      },
      "source": [
        "### Questions (4 marks)\n",
        "1. How do the training and validation accuracy change depending on the amount of data used? Explain with values.\n",
        "2. In this case, what do a false positive and a false negative represent? Which one is worse?\n",
        "\n",
        "*YOUR ANSWERS HERE*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Presenting potential security hazards for the user.\n",
        "\n",
        "Response 1 -\n",
        "\n",
        " Full Dataset (3450 samples): Training Accuracy: 92.87% Validation Accuracy: 93.83%. Initial Two Columns of Data (3450 samples): Training Accuracy: 60.84% Validation Accuracy: 61.30%.Just 5% of the Data (230 samples): Training Accuracy: 94.78% Validation Accuracy: 90.66%\n",
        "\n",
        "I noticed that the training accuracy tends to remain high even with a smaller dataset (comparing 92.87% with the full dataset versus 94.78% with 5% of the data), suggesting that the model can adapt well to the available data. However, validation accuracy may marginally decrease (from 93.83% to 90.66%) with smaller datasets,\n",
        "\n",
        "\n",
        "but it can still be relatively high if the subset used for validation is representative. Conversely, utilizing a limited subset of features (the first two columns) can result in a significant decrease in accuracy (Training Accuracy: 60.84% and Validation Accuracy: 61.30%), highlighting the importance of careful feature selection.\n",
        "\n",
        "Response 2 -\n",
        "\n",
        "\n",
        " False Positive: A false positive occurs when the spam filter mistakenly labels a legitimate email as spam. False Negative: A false negative takes place when the spam filter wrongly categorizes a spam email as legitimate.\n",
        "\n",
        "\n",
        " In my view, the consequence of a false negative is potentially graver. It signifies that unwanted spam emails go unfiltered and might clutter the user's inbox. Some of these spam emails might be phishing attempts or contain malware, presenting security risks to the user.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2BhJBGN0WXEB"
      },
      "id": "2BhJBGN0WXEB"
    },
    {
      "cell_type": "markdown",
      "id": "7559517a",
      "metadata": {
        "id": "7559517a"
      },
      "source": [
        "### Process Description (4 marks)\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*DESCRIBE YOUR PROCESS HERE*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A1:  The primary source of the code was the practice example provided by the instructor in D2L. Additionally, I utilized ChatGPT to gain insights into certain instructions.\n",
        "\n",
        "A2:\n",
        "1. I began by loading and organizing the data into a feature matrix and a target vector.\n",
        "2. Next, I checked for any missing values in the dataset.\n",
        "3. I selected the appropriate model class.\n",
        "4. I proceeded to instantiate the model and configure the hyperparameters.\n",
        "5. After that, I trained the model with the data.\n",
        "6. I used the trained model to make predictions for new data points.\n",
        "7. Finally, I assessed and compared the accuracy of the model.\n",
        "\n",
        "A3: I incorporated several prompts such as:\n",
        "\n",
        "  a. \"What is a tuple object, and can you provide an example of a tuple in a dataset?\"\n",
        "\n",
        "   b. \"How can I create a subset of a dataset?\" I adjusted the code accordingly.\n",
        "\n",
        "   c. \"How can I create a loop to insert data values into the dataframe?\"\n",
        "\n",
        "I had to make code adjustments as I encountered difficulties in extracting a subset of the data.\n",
        "\n",
        "A4:\n",
        " Yes, I encountered some challenges. For instance, I had difficulties with data splitting and understanding how regression models work. To overcome these challenges, I referred to lecture notes and examples available on D2L, which provided the necessary guidance and clarification for me to be successful."
      ],
      "metadata": {
        "id": "esjpBjfvdeOR"
      },
      "id": "esjpBjfvdeOR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "dzzmA-p-daMD"
      },
      "id": "dzzmA-p-daMD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "INjt0HX4Wnzk"
      },
      "id": "INjt0HX4Wnzk"
    },
    {
      "cell_type": "markdown",
      "id": "fb4c78a8",
      "metadata": {
        "id": "fb4c78a8"
      },
      "source": [
        "## Part 2: Regression (10.5 marks total)\n",
        "\n",
        "For this section, we will be evaluating concrete compressive strength of different concrete samples, based on age and ingredients. You will need to repeat the steps 1-4 from Part 1 for this analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ba83c5",
      "metadata": {
        "id": "b2ba83c5"
      },
      "source": [
        "### Step 1: Data Input (1 mark)\n",
        "\n",
        "The data used for this task can be downloaded using the yellowbrick library:\n",
        "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
        "\n",
        "Use the yellowbrick function `load_concrete()` to load the spam dataset into the feature matrix `X` and target vector `y`.\n",
        "\n",
        "Print the size and type of `X` and `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "6ff2e34f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ff2e34f",
        "outputId": "5867fe48-1189-4667-dc79-4f245d058335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of X:  (1030, 8)\n",
            "Type of X:  <class 'pandas.core.frame.DataFrame'>\n",
            "Size of y:  (1030,)\n",
            "Type of y:  <class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Import spam dataset from yellowbrick library\n",
        "# TO DO: Print size and type of X and y\n",
        "\n",
        "\n",
        "# Load the concrete dataset using Yellowbrick\n",
        "\n",
        "from yellowbrick.datasets import load_concrete\n",
        "\n",
        "# Load the dataset into variables X and y\n",
        "\n",
        "\n",
        "X, y = load_concrete()\n",
        "\n",
        "# Print the size and type of the feature matrix (X)\n",
        "\n",
        "print(\"Size of X: \", X.shape)\n",
        "\n",
        "print(\"Type of X: \", type(X))\n",
        "\n",
        "# Print the size and type of the target variable (y)\n",
        "\n",
        "print(\"Size of y: \", y.shape)\n",
        "\n",
        "print(\"Type of y: \", type(y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5294cfa",
      "metadata": {
        "id": "c5294cfa"
      },
      "source": [
        "### Step 2: Data Processing (0.5 marks)\n",
        "\n",
        "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "693c5fa3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "693c5fa3",
        "outputId": "81e0cde0-ff96-460b-f373-298e1d76421d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cement    0\n",
            "slag      0\n",
            "ash       0\n",
            "water     0\n",
            "splast    0\n",
            "coarse    0\n",
            "fine      0\n",
            "age       0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# TO DO: Check if there are any missing values and fill them in if necessary\n",
        "\n",
        "# Check for missing values in the DataFrame X\n",
        "anyMissingValue = X.isnull().sum()\n",
        "\n",
        "# Print the count of missing values\n",
        "print(anyMissingValue)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc60489",
      "metadata": {
        "id": "1bc60489"
      },
      "source": [
        "### Step 3: Implement Machine Learning Model (1 mark)\n",
        "\n",
        "1. Import `LinearRegression` from sklearn\n",
        "2. Instantiate model `LinearRegression()`.\n",
        "3. Implement the machine learning model with `X` and `y`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "b5041945",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "b5041945",
        "outputId": "8ebac9cc-e2ea-4ba6-dfd3-ce69d7539c11"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "# Note: for any random state parameters, you can use random_state = 0\n",
        "\n",
        "\n",
        "# Import necessary libraries and modules\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a Linear Regression model\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# Fit the Linear Regression model to the training data\n",
        "lr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1de28482",
      "metadata": {
        "id": "1de28482"
      },
      "source": [
        "### Step 4: Validate Model (1 mark)\n",
        "\n",
        "Calculate the training and validation accuracy using mean squared error and R2 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "970c038b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "970c038b",
        "outputId": "1cea39fe-30ec-4bc9-d2a3-1f963e7a1cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Training score: 111.36\n",
            "Mean Squared Validation score: 95.90\n",
            "\n",
            "R2 Training score: 0.61\n",
            "R2 Valdiation score: 0.62\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "\n",
        "# Calculate and print the Mean Squared Training score\n",
        "\n",
        "print(\"Mean Squared Training score: {:.2f}\".format(mean_squared_error(y_train, lr.predict(X_train))))\n",
        "\n",
        "# Calculate and print the Mean Squared Validation score\n",
        "\n",
        "print(\"Mean Squared Validation score: {:.2f}\".format(mean_squared_error(y_test, lr.predict(X_test))))\n",
        "\n",
        "# Leave a blank line for clarity\n",
        "\n",
        "print('')\n",
        "\n",
        "\n",
        "# Calculate and print the R2 Training score\n",
        "\n",
        "print(\"R2 Training score: {:.2f}\".format(r2_score(y_train, lr.predict(X_train))))\n",
        "\n",
        "\n",
        "# Calculate and print the R2 Validation score\n",
        "\n",
        "print(\"R2 Valdiation score: {:.2f}\".format(r2_score(y_test, lr.predict(X_test))))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54aa7795",
      "metadata": {
        "id": "54aa7795"
      },
      "source": [
        "### Step 5: Visualize Results (1 mark)\n",
        "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: MSE and R2 score\n",
        "2. Add the accuracy results to the `results` DataFrame\n",
        "3. Print `results`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "88d223f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88d223f3",
        "outputId": "3a6683fe-7686-4f98-e632-5f73516f8a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Training Accuracy  Validation Accuracy\n",
            "MSE                  111.36                95.90\n",
            "R2 score               0.61                 0.62\n"
          ]
        }
      ],
      "source": [
        "# TO DO: ADD YOUR CODE HERE\n",
        "\n",
        "# Define a dictionary 'data' to store training and validation accuracy metrics\n",
        "\n",
        "data = {\n",
        "\n",
        "  'Training Accuracy' : [round(mean_squared_error(y_train, lr.predict(X_train)), 2), round(r2_score(y_train, lr.predict(X_train)), 2)],\n",
        "\n",
        "  'Validation Accuracy' : [round(mean_squared_error(y_test, lr.predict(X_test)), 2), round(r2_score(y_test, lr.predict(X_test)), 2) ]\n",
        "\n",
        "}\n",
        "\n",
        "# Create a Pandas DataFrame 'results' from the 'data' dictionary with custom index\n",
        "\n",
        "results = pd.DataFrame(data, index = ['MSE', 'R2 score'])\n",
        "\n",
        "\n",
        "# Print the DataFrame 'results' to display the accuracy metrics\n",
        "\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70a42bda",
      "metadata": {
        "id": "70a42bda"
      },
      "source": [
        "### Questions (2 marks)\n",
        "1. Did using a linear model produce good results for this dataset? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Answer:-\n",
        "\n",
        "\n",
        "In this instance, the performance of the linear model was subpar. The Mean Squared Error (MSE) during training stood at approximately 111.36, while the validation MSE was around 95.90. In terms of R2 scores, which assess how well the model accounts for the data, the values were only about 0.61 for training and 0.62 for validation.\n",
        "\n",
        "These figures indicate that the linear model encountered challenges in making precise predictions. The elevated MSE values suggest that the model's predictions deviated significantly from the actual values, and the R2 scores signify that it could only elucidate around 60% of the data's variability.\n",
        "\n",
        "Hence, employing a linear model did not yield favorable outcomes for this dataset. It might be advisable to explore alternative models or strategies in order to enhance accuracy."
      ],
      "metadata": {
        "id": "VBbrIGIyXSKU"
      },
      "id": "VBbrIGIyXSKU"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jENiczI5XR2-"
      },
      "id": "jENiczI5XR2-"
    },
    {
      "cell_type": "markdown",
      "id": "2ca0ff2f",
      "metadata": {
        "id": "2ca0ff2f"
      },
      "source": [
        "### Process Description (4 marks)\n",
        "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
        "1. Where did you source your code?\n",
        "1. In what order did you complete the steps?\n",
        "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
        "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*DESCRIBE YOUR PROCESS HERE*\n",
        "\n",
        "\n",
        "Answer 1:\n",
        "\n",
        "The main source of the code was the practice example given by the instructor in D2L. In addition, I used ChatGPT to learn some of the instructions.\n",
        "\n",
        "Answer 2:\n",
        "1. Loading and arrange data into feature matrix and target vector\n",
        "2. Checking any missing values\n",
        "3. Choose model class\n",
        "4. Instantiating model and selecting hyperparameters\n",
        "5.Fitting model to data\n",
        "6.Predict values for new data\n",
        "7. Comparing accuracy.\n",
        "\n",
        "Answer 3 :\n",
        "\n",
        "  I used some of the prompts like:-What are mean square error and R2 Score and how with these parameters define the model\n",
        "\n",
        "Answer 4 :   \n",
        "\n",
        " Yes, I faced some difficulties like I was not able to understand R2 score then I took help from google as well..\n"
      ],
      "metadata": {
        "id": "orrNj5cHXZJO"
      },
      "id": "orrNj5cHXZJO"
    },
    {
      "cell_type": "markdown",
      "id": "dfdb0880",
      "metadata": {
        "id": "dfdb0880"
      },
      "source": [
        "*DESCRIBE YOUR PROCESS HERE*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e72ac3eb",
      "metadata": {
        "id": "e72ac3eb"
      },
      "source": [
        "## Part 3: Observations/Interpretation (3 marks)\n",
        "\n",
        "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
        "\n",
        "\n",
        "*ADD YOUR FINDINGS HERE*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Part 1 - Full Data Set vs. First Two Columns:\n",
        "When using the entire dataset, we observe a notable increase in both training and validation accuracy, demonstrating strong performance. Conversely, when restricting the analysis to just the first two columns, there is a significant decrease in accuracy, pointing to the necessity for more data or more informative features.\n",
        "\n",
        "Full Data Set vs. 5% of the Data:\n",
        "Surprisingly, even with just 5% of the dataset, training accuracy remains quite high. Although validation accuracy is slightly lower, it remains at a satisfactory level, suggesting that this smaller subset might contain crucial patterns worth exploring.\n",
        "\n",
        "Part 2 - \"Upon examining the outcomes, a distinct trend becomes evident. The Mean Squared Error (MSE) values for both the training and validation datasets are relatively elevated, with the training data registering around 111.36 and the validation data around 95.90. Furthermore, the R2 scores, which gauge the model's goodness of fit to the data, hover around 0.61 for training and 0.62 for validation. These results align with our classroom discussions. A high MSE signifies the linear model's difficulty in making accurate predictions, and while the R2 scores are not exceedingly low, they indicate that the model could only account for roughly 60% of the variance within the data.\"\n"
      ],
      "metadata": {
        "id": "fH5s1DrXXfPD"
      },
      "id": "fH5s1DrXXfPD"
    },
    {
      "cell_type": "markdown",
      "id": "40b84eed",
      "metadata": {
        "id": "40b84eed"
      },
      "source": [
        "## Part 4: Reflection (2 marks)\n",
        "Include a sentence or two about:\n",
        "- what you liked or disliked,\n",
        "- found interesting, confusing, challangeing, motivating\n",
        "while working on this assignment.\n",
        "\n",
        "\n",
        "*ADD YOUR THOUGHTS HERE*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I appreciated the structure of this assignment. When creating it, I had the opportunity to revisit all the elements we've studied. The most demanding aspect was addressing the sections related to MSE and R2 scores."
      ],
      "metadata": {
        "id": "0ilxO5AHXneh"
      },
      "id": "0ilxO5AHXneh"
    },
    {
      "cell_type": "markdown",
      "id": "db951b3a",
      "metadata": {
        "id": "db951b3a"
      },
      "source": [
        "## Part 5: Bonus Question (4 marks)\n",
        "\n",
        "Repeat Part 2 with Ridge and Lasso regression to see if you can improve the accuracy results. Which method and what value of alpha gave you the best R^2 score? Is this score \"good enough\"? Explain why or why not.\n",
        "\n",
        "**Remember**: Only test values of alpha from 0.001 to 100 along the logorithmic scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "47623d44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47623d44",
        "outputId": "e3c92d4d-48e1-45b5-c67d-b4c34c66f0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 Training score for Ridge model: 0.61\n",
            "R2 Valdiation score for Ridge model: 0.62\n",
            "R2 Training score for Lasso model: 0.61\n",
            "R2 Valdiation score for Lasso model: 0.62\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Importing necessary libraries and modules\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Creating a Ridge regression model with alpha=1\n",
        "\n",
        "ridge = Ridge(alpha=1)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "\n",
        "\n",
        "# Fitting the Ridge model to the training data\n",
        "\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "# Printing the R2 score for the training set of the Ridge model\n",
        "\n",
        "\n",
        "print(\"R2 Training score for Ridge model: {:.2f}\".format(r2_score(y_train, ridge.predict(X_train))))\n",
        "\n",
        "# Printing the R2 score for the testing set of the Ridge model\n",
        "\n",
        "print(\"R2 Valdiation score for Ridge model: {:.2f}\".format(r2_score(y_test, ridge.predict(X_test))))\n",
        "\n",
        "\n",
        "\n",
        "# Importing the Lasso regression model\n",
        "\n",
        "\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "# Creating a Lasso regression model with alpha=0.001\n",
        "\n",
        "\n",
        "lasso = Lasso(alpha=0.001)\n",
        "\n",
        "# Fitting the Lasso model to the training data\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
        "\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Printing the R2 score for the training set of the Lasso model\n",
        "\n",
        "print(\"R2 Training score for Lasso model: {:.2f}\".format(r2_score(y_train, lasso.predict(X_train))))\n",
        "\n",
        "# Printing the R2 score for the testing set of the Lasso model\n",
        "\n",
        "print(\"R2 Valdiation score for Lasso model: {:.2f}\".format(r2_score(y_test, lasso.predict(X_test))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b606236",
      "metadata": {
        "id": "1b606236"
      },
      "source": [
        "*ANSWER HERE*\n",
        "\n",
        "Answer :\n",
        "In practice, a favorable R^2 score typically falls in the range of 0.7 to 0.8 or higher, signifying the model's ability to explain a significant portion of the data's variability. Nevertheless, when using the Ridge model with alpha values ranging from 0.01 to 37, its highest attainable R^2 value is only around 0.54, and the same holds true for the Lasso model at alpha = 0.01. However, when comparing these models to Linear Regression, it becomes evident that neither of them yields a satisfactory performance."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}